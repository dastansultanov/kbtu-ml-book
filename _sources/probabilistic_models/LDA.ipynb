{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbfeca2d",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a254c985",
   "metadata": {},
   "source": [
    "## Introduction to LDA\n",
    "\n",
    "**Linear Discriminant Analysis (LDA)** is a statistical method used for dimensionality reduction and classification in machine learning. At its core, LDA seeks to find the linear combinations of features that best separate different classes in a dataset. This allows for efficient representation of data while maximizing class separability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288303bb",
   "metadata": {},
   "source": [
    "### Historical Context\n",
    "\n",
    "Introduced by Ronald A. Fisher in the 1930s, LDA has since become a fundamental tool in the fields of statistics and machine learning. Fisher formulated LDA as a means to find the linear combination of features that maximizes the ratio of between-class variance to within-class variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f1d10e",
   "metadata": {},
   "source": [
    "## Theoretical Foundations of LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e349609",
   "metadata": {},
   "source": [
    "### Mathematical Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b66790",
   "metadata": {},
   "source": [
    "#### Objective Function\n",
    "\n",
    "The primary goal of LDA is to maximize the between-class scatter while minimizing the within-class scatter. The objective function can be expressed as:\n",
    "$$\n",
    "J(W) = \\frac{{\\text{{det}}(W^{-1}B)}}{{\\text{{det}}(W^{-1}W)}} \n",
    "$$\n",
    "\n",
    "where $(W)$ is the weight matrix, $(B)$ is the between-class scatter matrix, and $(W^{-1})$ denotes the inverse of $(W)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b73fe3",
   "metadata": {},
   "source": [
    "#### Eigenvalue Decomposition\n",
    "\n",
    "To maximize the objective function, LDA involves the eigenvalue decomposition of \\(W^{-1}B\\). The eigenvectors corresponding to the largest eigenvalues form the basis for the discriminant subspace.\n",
    "\n",
    "$$ W^{-1}Bv_i = \\lambda_i v_i $$\n",
    "\n",
    "where $(v_i)$ is the $(i)$-th eigenvector, and $(\\lambda_i)$ is the corresponding eigenvalue.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b80412",
   "metadata": {},
   "source": [
    "### Assumptions and Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da84a98e",
   "metadata": {},
   "source": [
    "#### Assumptions\n",
    "1. **Multivariate Normality:** LDA assumes that the features in each class follow a multivariate normal distribution.\n",
    "2. **Equality of Covariance Matrices:** It assumes that the covariance matrices of different classes are equal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d69c6db",
   "metadata": {},
   "source": [
    "#### Limitation\n",
    "1. **Sensitive to Outliers:** LDA can be sensitive to outliers, impacting its performance.\n",
    "2. **Assumption Violations:** If the assumptions are violated, the effectiveness of LDA may be compromised."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced06205",
   "metadata": {},
   "source": [
    "```{warning}\n",
    "Considering these limitatuins is essential for practitioners to use LDA effectively and understand potential challenges or issues that may arise when applying the method to real-world datasets.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0d2092",
   "metadata": {},
   "source": [
    "## Purpose of LDA\n",
    "\n",
    "LDA serves a dual purpose: it reduces the dimensionality of the data while retaining information relevant for classification, making it a versatile technique in machine learning applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c00ad34",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction with LDA\n",
    "\n",
    "In high-dimensional spaces, the distance between data points increases, making algorithms prone to overfitting. Working with lower-dimensional data can lead to more computationally efficient models. Therefore, reduced dimensionality makes it easier to visualize data and its inherent patterns. \n",
    "LDA is often used for dimensionality reduction. The primary goal is to transform the original feature space into a lower-dimensional space while preserving the discriminatory information between classes. In other words, LDA looks for a projection of the data in a way that maximizes the separation between different classes.\n",
    "\n",
    "### Objectives of Dimensionality Reduction\n",
    "\n",
    "1. LDA aims to **maximize the distance between the means** of different classes, enhancing class separability.\n",
    "\n",
    "2. Simultaneously, LDA seeks to **minimize the scatter within each class**, ensuring compact clusters.\n",
    "\n",
    "```{admonition} Note\n",
    ":class: note\n",
    "LDA accomplishes these objectives by finding a subspace that best captures the essential information for classification.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3567dd",
   "metadata": {},
   "source": [
    "## Classification using LDA\n",
    "\n",
    "Once LDA has been applied for dimensionality reduction, it can be employed for classification tasks. The process involves training the model on a labeled dataset, where the class labels are known, and learning the discriminative patterns in the reduced-dimensional space.\n",
    "\n",
    "### Working Principle of LDA in Classification\n",
    "\n",
    "1. LDA aims to find decision boundaries that best separate different classes in the reduced space.\n",
    "\n",
    "2. LDA can be interpreted as a probabilistic model. It models the distribution of features for each class and uses Bayes' theorem to calculate the probability of a data point belonging to a particular class.\n",
    "\n",
    "3. Given a set of features for an unseen data point, LDA can predict the most likely class based on the learned discriminative patterns during training.\n",
    "\n",
    "### Applications of LDA in Classification\n",
    "\n",
    "- **Face Recognition:**\n",
    "  LDA is commonly used for face recognition tasks where reducing the dimensionality of facial features enhances the accuracy of recognition.\n",
    "\n",
    "- **Medical Diagnosis:**\n",
    "  In medical diagnostics, LDA can help classify patients into different diagnostic categories based on relevant features.\n",
    "\n",
    "- **Speech Recognition:**\n",
    "  LDA can be applied to features extracted from speech signals to classify spoken words or phrases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffef24d",
   "metadata": {},
   "source": [
    "## Practical Applications of LDA\n",
    "\n",
    "### Real-world Examples\n",
    "\n",
    "#### Facial Recognition Systems:\n",
    "In computer vision, LDA is applied to extract discriminative features for facial recognition. By capturing the essential characteristics that differentiate faces, LDA enhances the accuracy of recognition systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c46a54",
   "metadata": {},
   "source": [
    "\n",
    "```{admonition} Industry Use Cases\n",
    ":class: tip, dropdown \n",
    "\n",
    "Fraud Detection in Banking\n",
    "\n",
    "Linear Discriminant Analysis (LDA) proves to be a valuable tool in the banking industry, particularly in the realm of fraud detection. Here's how LDA is applied to address the challenges associated with identifying fraudulent transactions:\n",
    "\n",
    "*Banks face the constant challenge of distinguishing between legitimate and fraudulent transactions within vast datasets. The goal is to detect unusual patterns or anomalies that may indicate fraudulent activities.\n",
    "\n",
    "*LDA helps model the characteristics of legitimate and fraudulent transactions by analyzing the underlying patterns in the data. It aims to maximize the separation between different classes, making it effective in distinguishing between normal and potentially fraudulent behavior.\n",
    "\n",
    "* Let's denote the features of the transactions as $(X)$ and the corresponding class labels as $(y)$. The primary objective is to find the linear combination of features that maximizes the distance between the means of different classes while minimizing the spread within each class. This is achieved through the optimization of the LDA objective function.\n",
    "\n",
    "*In a practical scenario, the transaction data would be preprocessed, ensuring proper handling of missing values and scaling of features. Next, you'd apply LDA to compute the discriminant functions, leveraging the covariance matrices to identify the most discriminative directions in the data.\n",
    "\n",
    "*The application of LDA in fraud detection enables the creation of a model that can identify potentially fraudulent transactions based on their deviation from the learned patterns. This contributes to enhancing the security and integrity of banking systems.\n",
    "\n",
    "By leveraging LDA, banks can significantly improve their ability to detect and prevent fraudulent activities, ultimately safeguarding the interests of both the financial institution and its customers.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08474587",
   "metadata": {},
   "source": [
    "# Step by Step Guide to Implementing LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d4f3d2",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89368abb",
   "metadata": {},
   "source": [
    "```{warning}\n",
    "Before applying LDA, ensure proper data preprocessing.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6babb0",
   "metadata": {},
   "source": [
    "1. **Handling Missing Values**\n",
    "   - Address any missing values in the dataset. Fill in missing values or remove instances with missing data to ensure a complete dataset.\n",
    "\n",
    "2. **Scaling Features**\n",
    "   - Standardize or normalize features. This step ensures that all features contribute equally to the analysis, preventing any particular feature from dominating the model due to its scale.\n",
    "\n",
    "3. **Checking Assumptions**\n",
    "   - Verify that your data meets LDA assumptions. LDA assumes that the features in each class follow a multivariate normal distribution, and the covariance matrices of different classes are equal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63f967a",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57b354a",
   "metadata": {},
   "source": [
    "```{admonition} Where the fun begins\n",
    ":class: note \n",
    "Now, let's delve into the details of training your LDA model.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2276131",
   "metadata": {},
   "source": [
    "#### 1. Compute Covariance Matrices:\n",
    "\n",
    "The within-class covariance matrix $(S_W)$ and the between-class covariance matrix $(S_B)$ are calculated as follows:\n",
    "\n",
    "- Within-class covariance matrix $(S_W)$:\n",
    "\n",
    "  $$S_W = \\sum_{i=1}^{c} \\sum_{j=1}^{n_i} (x_{ij} - \\mu_i)(x_{ij} - \\mu_i)^T$$\n",
    " \n",
    "  where $(c)$ is the number of classes, $(n_i)$ is the number of samples in class $(i)$, $(x_{ij})$ is the $(j)$-th sample from class $(i)$, and $(\\mu_i)$ is the mean vector of class $(i)$.\n",
    "\n",
    "- Between-class covariance matrix $(S_B)$:\n",
    "  \n",
    "  $$S_B = \\sum_{i=1}^{c} n_i (\\mu_i - \\mu)(\\mu_i - \\mu)^T$$\n",
    "  \n",
    "  where $(c)$ is the number of classes, $(n_i)$ is the number of samples in class $(i)$, $(\\mu_i)$ is the mean vector of class $(i)$, and $(\\mu)$ is the overall mean vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5375a6",
   "metadata": {},
   "source": [
    "#### 2. Solve Eigenvalue Problem:\n",
    "\n",
    "Next, solve the eigenvalue problem to obtain the eigenvalues $\\lambda$ and corresponding eigenvectors $(v)$ of the matrix $(S_W^{-1}S_B)$. This involves solving the following equation:\n",
    "  \n",
    "  $$S_W^{-1}S_Bv = \\lambda v$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bb519b",
   "metadata": {},
   "source": [
    "#### 3. Select Discriminant Functions:\n",
    "\n",
    "Choose the top $(k)$ eigenvectors corresponding to the $(k)$ largest eigenvalues to form the matrix $(W)$. The discriminant functions can be defined as:\n",
    "\n",
    "  $$Y = XW$$ \n",
    "\n",
    "  where $(Y)$ is the transformed feature matrix, $(X)$ is the original feature matrix, and $(W)$ contains the selected eigenvectors.\n",
    "\n",
    "These discriminant functions will serve as the basis for making predictions and defining decision boundaries in your LDA model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74489122",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "After training your LDA model, it's crucial to assess its performance using various evaluation metrics. These metrics provide insights into how well the model generalizes to new, unseen data. Let's explore some commonly used evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3c274b",
   "metadata": {},
   "source": [
    "#### 1. Accuracy\n",
    "\n",
    "Accuracy is the most straightforward metric, representing the ratio of correctly predicted instances to the total number of instances. It is calculated as follows:\n",
    "  \n",
    "  $$\\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ad185e",
   "metadata": {},
   "source": [
    "#### 2. Precision\n",
    "\n",
    "Precision focuses on the accuracy of positive predictions. It is the ratio of correctly predicted positive instances to the total predicted positives, and it is calculated as follows:\n",
    "  \n",
    "  $$\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives + False Positives}}$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52253b07",
   "metadata": {},
   "source": [
    "#### 3. Recall (Sensitivity)\n",
    "\n",
    "Recall, also known as sensitivity or true positive rate, measures the ability of the model to identify all relevant instances. It is calculated as the ratio of true positives to the total actual positives:\n",
    "  \n",
    "  $$\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives + False Negatives}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f0e7cd",
   "metadata": {},
   "source": [
    "#### 4. F1 Score\n",
    "\n",
    "The F1 score is the harmonic mean of precision and recall, providing a balanced measure between precision and recall. It is calculated as follows:\n",
    "  \n",
    "  $$\\text{F1 Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision + Recall}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7890abca",
   "metadata": {},
   "source": [
    "```{admonition} Remember\n",
    ":class: Warning \n",
    "Evaluating your model using these metrics is crucial to understanding its strengths and weaknesses, guiding potential improvements, and ensuring its suitability for real-world applications.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b65d690",
   "metadata": {},
   "source": [
    "## Code for LDA Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3a2029",
   "metadata": {},
   "source": [
    "This example assumes you have preprocessed your data, loaded features $(X)$ and labels $(y)$, and split the dataset into training and testing sets. It then applies Linear Discriminant Analysis **(LDA)** and uses a simple Logistic Regression classifier for training and evaluation. The evaluation metrics include accuracy, precision, recall, and F1 score, providing a comprehensive assessment of the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9181ac53",
   "metadata": {},
   "source": [
    "```{admonition} Try it\n",
    ":class: tip\n",
    "Feel free to adapt the code to your specific dataset and requirements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5210d9d1",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Your data preprocessing and loading here\n",
    "# Assume you have X (features) and y (labels) loaded and preprocessed\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply LDA\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "X_train_lda = lda.fit_transform(X_train, y_train)\n",
    "X_test_lda = lda.transform(X_test)\n",
    "\n",
    "# Your model training and evaluation code here\n",
    "# For simplicity, let's use a basic classifier like Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Instantiate and train the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_lda, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test_lda)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a265a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
